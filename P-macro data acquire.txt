import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import pandas as pd
import json
import numpy as np


# ----------------------GET DATA HISTORICAL 10VNBY

url_10 = "https://www.worldgovernmentbonds.com/wp-admin/admin-ajax.php?action=jsonStoricoBond&area=58&dateRif=2099-12-31&durata=120&key=d1RHRzY4aE16dU9TaXRvSndPWVBEdz09"

response_10 = requests.get(url_10).text
data_10 = json.loads(response_10)  # Parse the JSON data directly
# print(data_dict)

# Convert to DataFrame
df_10 = pd.DataFrame.from_dict(data_10['quote'], orient='index', columns=['date', 'bond_10y'])

# Convert timestamp to readable date format
df_10['date'] = pd.to_datetime(df_10['date'], unit='ms')


df_combined_10f = pd.concat([df_10], ignore_index=True).dropna(subset=['bond_10y'])

# REMOVE DUPLICATE LIST
df_combined_10f = df_combined_10f.drop_duplicates(subset=['date'], keep='last')

# ----------------------GET DATA FORECAST 10VNBY


url_10f = ('http://www.worldgovernmentbonds.com/bond-forecast/vietnam/10-years/')
r_10f = requests.get(url_10f)
# print(r.text)

# Get time forecast
web_content_10f = BeautifulSoup(r_10f.text, 'lxml')
web_content_10f = web_content_10f.find('div', {"class" : 'w3-responsive'})
web_content_10f = web_content_10f.find('thead').text
# print(web_content)

# Use regular expression to find lines with dates in MMM YYYY format
pattern = r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) \d{4}'
dates_10f = re.findall(pattern, web_content_10f)

# Convert month-year strings to datetime objects with day set to 1
date_objects_10f = [datetime.strptime(date, '%b %Y') for date in dates_10f]

date_10f = []
# Print the converted date objects
for date_obj_10f in date_objects_10f:
    date_10f.append(date_obj_10f)

date_10f = pd.DataFrame({'date': date_10f})
# print(date)

# Get number forecast
web_number_10f = BeautifulSoup(r_10f.text, 'lxml')
web_number_10f = web_number_10f.find('div', {"class" : 'w3-responsive'})
web_number_10f = web_number_10f.find('tbody')
web_number_10f = web_number_10f.find('tr').text

# # Use regular expression to find lines with '%'
pattern = r'^\d+\.\d+%$'
lines_with_percent_10f = re.findall(pattern, web_number_10f, re.MULTILINE)

# Print the matching lines
bond_y_10f = []
for line in lines_with_percent_10f:
    bond_y_10f.append(line)

bond_y_10f = pd.DataFrame({'bond_10y': bond_y_10f})
# print(bond_y)

# Consolidate the two DataFrames into one
consolidated_df_10f = pd.concat([date_10f, bond_y_10f], axis=1)

# Remove the '%' symbol
consolidated_df_10f['bond_10y'] = consolidated_df_10f['bond_10y'].str.replace('%', '')

# Convert the 'bond_y' column to numeric
consolidated_df_10f['bond_10y'] = pd.to_numeric(consolidated_df_10f['bond_10y'])

# COMBINE DATAFRAME

df_combined_10f = pd.concat([df_combined_10f, consolidated_df_10f], ignore_index=True).dropna(subset=['bond_10y'])

# REMOVE DUPLICATE LIST
df_combined_10f = df_combined_10f.drop_duplicates(subset=['date'], keep='last')

# ----------------------GET DATA HISTORICAL 02VNBY

url_2 = "https://www.worldgovernmentbonds.com/wp-admin/admin-ajax.php?action=jsonStoricoBond&area=58&dateRif=2099-12-31&durata=24&key=dENuNGVDdERQdUh6ZEhqZTBOWnN5Zz09"

response_2 = requests.get(url_2).text
data_2 = json.loads(response_2)  # Parse the JSON data directly
# print(data_dict)

# Convert to DataFrame
df_2 = pd.DataFrame.from_dict(data_2['quote'], orient='index', columns=['date', 'bond_2y'])

# Convert timestamp to readable date format
df_2['date'] = pd.to_datetime(df_2['date'], unit='ms')

df_combined_2f = pd.concat([df_2], ignore_index=True).dropna(subset=['bond_2y'])

# REMOVE DUPLICATE LIST
df_combined_2f = df_combined_2f.drop_duplicates(subset=['date'], keep='last')

# ----------------------GET DATA FORECAST 2VNBY


url_2f = ('http://www.worldgovernmentbonds.com/bond-forecast/vietnam/2-years/')
r_2f = requests.get(url_2f)
# print(r.text)

# Get time forecast
web_content_2f = BeautifulSoup(r_2f.text, 'lxml')
web_content_2f = web_content_2f.find('div', {"class" : 'w3-responsive'})
web_content_2f = web_content_2f.find('thead').text
# print(web_content)

# Use regular expression to find lines with dates in MMM YYYY format
pattern = r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) \d{4}'
dates_2f = re.findall(pattern, web_content_2f)

# Convert month-year strings to datetime objects with day set to 1
date_objects_2f = [datetime.strptime(date, '%b %Y') for date in dates_2f]

date_2f = []
# Print the converted date objects
for date_obj_2f in date_objects_2f:
    date_2f.append(date_obj_2f)

date_2f = pd.DataFrame({'date': date_2f})
# print(date)

# Get number forecast
web_number_2f = BeautifulSoup(r_2f.text, 'lxml')
web_number_2f = web_number_2f.find('div', {"class" : 'w3-responsive'})
web_number_2f = web_number_2f.find('tbody')
web_number_2f = web_number_2f.find('tr').text

# # Use regular expression to find lines with '%'
pattern = r'^\d+\.\d+%$'
lines_with_percent_2f = re.findall(pattern, web_number_2f, re.MULTILINE)

# Print the matching lines
bond_y_2f = []
for line in lines_with_percent_2f:
    bond_y_2f.append(line)

bond_y_2f = pd.DataFrame({'bond_2y': bond_y_2f})
# print(bond_y)

# Consolidate the two DataFrames into one
consolidated_df_2f = pd.concat([date_2f, bond_y_2f], axis=1)

# Remove the '%' symbol
consolidated_df_2f['bond_2y'] = consolidated_df_2f['bond_2y'].str.replace('%', '')

# Convert the 'bond_y' column to numeric
consolidated_df_2f['bond_2y'] = pd.to_numeric(consolidated_df_2f['bond_2y'])

# COMBINE DATAFRAME

df_combined_2f = pd.concat([df_combined_2f, consolidated_df_2f], ignore_index=True).dropna(subset=['bond_2y'])

# REMOVE DUPLICATE LIST
df_combined_2f = df_combined_2f.drop_duplicates(subset=['date'], keep='last')

# --------------------COMBINE DATAFRAME

# JOIN 2 DATAFRAME
df_combined = pd.merge(df_combined_10f, df_combined_2f, how="left"\
              , left_on=['date']\
              , right_on=['date']\
              , validate="1:1")

# -----------------------GET DATA OMO

# Load your data
omo_df = pd.read_csv('C:\\Users\\417776\\Downloads\\Python\\fin\\macro data\\OMO.csv')
omo_df['date'] = pd.to_datetime(omo_df['Date'])
omo_df = omo_df[['date', 'OMO', 'OMO_balance']]

# Load your data update
omo_df2 = pd.read_excel('C:\\Users\\417776\\Downloads\\Python\\fin\\macro data\\OMO_update.xlsx', sheet_name = 'OMO_update')    
omo_df2['date'] = pd.to_datetime(omo_df2['Date'])
omo_df2['OMO'] = omo_df2['Trans_amt']

omo_df2['OMO_balance'] = omo_df2['Balance']

omo_df2 = omo_df2[['date', 'OMO', 'OMO_balance']]


# COMBINE DATAFRAME
combined_omo_df = pd.concat([omo_df, omo_df2], ignore_index=True)

# REMOVE DUPLICATE LIST
combined_omo_df = combined_omo_df.drop_duplicates(subset=['date'], keep='last')

# Calculate cumulative sum
combined_omo_df['OMO_cum'] = np.cumsum(combined_omo_df['OMO'])

# JOIN 2 DATAFRAME
df_combined3 = pd.merge(df_combined, combined_omo_df, how="left"\
              , left_on=['date']\
              , right_on=['date']\
              , validate="1:1")

df_combined3['OMO_balance'] = df_combined3['OMO_balance'].fillna(method='ffill').fillna(method='bfill')
df_combined3['OMO_cum'] = df_combined3['OMO_cum'].fillna(method='ffill').fillna(method='bfill')
    

# -----------------------GET DATA VNDUSD

from pycoingecko import CoinGeckoAPI

cg = CoinGeckoAPI()

# Get a list of all supported coins
coins_list = cg.get_coins_list()

# Filter coins based on symbol
coins_list = filter(lambda coin: coin['id'] in ["usd"], coins_list)    

# Initialize an empty list to store OHLC data
all_coin_data = []

# Loop through each coin
for coin in coins_list:
    coin_id = coin['id']
    coin_symbol = coin['symbol']

    # Get OHLC data for the past 14 days (adjust as needed)
    ohlc = cg.get_coin_ohlc_by_id(id=coin_id, vs_currency='vnd', days='365')

    # Create a dataframe for the coin's OHLC data
    ohlc_df = pd.DataFrame(ohlc, columns=["date", "open", "high", "low", "close"])
    ohlc_df["date"] = pd.to_datetime(ohlc_df["date"], unit="ms")
    ohlc_df["symbol"] = coin_symbol
    # print(ohlc_df)

    # Append to the list of all coin data
    all_coin_data.append(ohlc_df)

# Concatenate dataframes for all coins
forex_pd = pd.concat(all_coin_data, ignore_index=True)
forex_pd['VNDUSD'] = forex_pd['close']
forex_pd = forex_pd[['date', 'VNDUSD']]


# # JOIN 2 DATAFRAME
# df_combined2 = pd.merge(df_combined, forex_pd, how="left"\
#               , left_on=['date']\
#               , right_on=['date']\
#               , validate="1:1")

# GET DATA VNDUSD_excel
# Load your data
vndusd_df = pd.read_csv('C:\\Users\\417776\\Downloads\\Python\\fin\\macro data\\VNDUSD_consol.csv')
vndusd_df['date'] = pd.to_datetime(vndusd_df['date'])
vndusd_df = vndusd_df[['date', 'VNDUSD']]

# COMBINE DATAFRAME
combined_vndusd_df = pd.concat([forex_pd, vndusd_df], ignore_index=True).dropna(subset=['VNDUSD'])

# REMOVE DUPLICATE LIST
combined_vndusd_df = combined_vndusd_df.drop_duplicates(subset=['date'], keep='last')

# JOIN 2 DATAFRAME
df_combined3 = pd.merge(df_combined3, combined_vndusd_df, how="left"\
              , left_on=['date']\
              , right_on=['date']\
              , validate="1:1")

df_combined3['VNDUSD'] = df_combined3['VNDUSD'].fillna(method='ffill').fillna(method='bfill')
    
# -----------------------GET DATA VNINT_consol

# Load your data
vnint_df = pd.read_csv('C:\\Users\\417776\\Downloads\\Python\\fin\\macro data\\VNInterest_consol.csv')
vnint_df['date'] = pd.to_datetime(vnint_df['date'])
vnint_df = vnint_df[['date', 'VN_INT_3M']]

# JOIN 2 DATAFRAME
df_combined3 = pd.merge(df_combined3, vnint_df, how="left"\
              , left_on=['date']\
              , right_on=['date']\
              , validate="1:1")

df_combined3['VN_INT_3M'] = df_combined3['VN_INT_3M'].fillna(method='ffill').fillna(method='bfill')

# -----------------------GET DATA VNINDEX

# Load your data update
index_df = pd.read_excel('C:\\Users\\417776\\Downloads\\Python\\fin\\macro data\\vnin.xlsx', sheet_name = 'Sheet1')    
index_df['date'] = pd.to_datetime(index_df['Date/Time'])
index_df = index_df[['date', 'index']]

# JOIN 2 DATAFRAME
df_combined3 = pd.merge(df_combined3, index_df, how="left"\
              , left_on=['date']\
              , right_on=['date']\
              , validate="1:1")

df_combined3['index'] = df_combined3['index'].fillna(method='ffill').fillna(method='bfill')

# Filter
# df_combined3 = df_combined3[df_combined3.date >= "16-09-2019"]
df_combined3 = df_combined3[df_combined3.date >= "22-09-2022"]
df_combined3['bond_gap'] = df_combined3['bond_10y']-df_combined3['bond_2y']

# Print the consolidated DataFrame
print(df_combined3)
df_combined3.to_csv("sample1.csv")





# --------------------------------------------------------Non-Linear regression to predict





import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from sklearn.inspection import partial_dependence
import numpy as np

data_file = r'C:\Users\417776\Downloads\Python\fin\macro data\sample1.csv'


# Load your DataFrame (replace 'your_data.csv' with your actual data file)
df = pd.read_csv(data_file)
# df = df.head(2000)
print(df)

# Separate features (X) and target (y)
X = df[['bond_gap', 'VNDUSD', 'VN_INT_3M' , 'OMO_cum' ]]
y = df['index']
# print(X)

# Initialize Random Forest model
model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)

# Walk-forward validation
tscv = TimeSeriesSplit(n_splits=5)
for train_index, test_index in tscv.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    print(f"Fold MSE: {mse:.4f}")

# Train the final model on the entire dataset
model.fit(X, y)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate model performance (optional)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.2f}")

# Print the most important features
important_features_dict = {}
for idx, val in enumerate(model.feature_importances_):
    important_features_dict[idx] = val

important_features_list = sorted(important_features_dict, key=important_features_dict.get, reverse=True)
print(f'The most important features are: {important_features_list}')

# Print all the attributes
print(f'The coefficient of determination R^2 of the prediction is: {model.score(X_test, y_test)}')
# print(f'The number of trees in the forest is: {model.n_estimators}')
# print(f'The function to measure the quality of a split is: {model.criterion}')
# print(f'The maximum depth of the tree is: {model.max_depth}')
# print(f'The minimum number of samples required to split an internal node is: {model.min_samples_split}')
# print(f'The minimum number of samples required to be at a leaf node is: {model.min_samples_leaf}')
# print(f'The output for a given input is: {model.predict(X_test)}')
# print(f'The list of decision trees in the forest is: {model.estimators_}')

# ------------Sensitivity analysis
# Drop duplicate rows
X_copy = X.copy()
X_unique = X_copy.drop_duplicates()

# Predict RenewalRate for each row in X_unique
predictions_index = model.predict(X_unique)

# # Add the predicted RenewalRate values to X_unique
X_unique['predicted_index'] = predictions_index

print(X_unique)
X_unique.to_csv("predicted_index.csv")

# # Visualize relationships (optional)
# plt.scatter(X_test['VNDUSD'], y_test, label='index')
# plt.scatter(X_test['VNDUSD'], y_pred, label='predicted_index', color='red', marker='x')
# plt.xlabel('VNDUSD')
# plt.ylabel('index')
# plt.title('VNDUSD vs. index')
# plt.legend()
# plt.show()


# Visual Scatter plot of y_test vs y_pred
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel('Actual values (y_test)')
plt.ylabel('Predicted values (y_pred)')
plt.title('Actual vs Predicted values')

# Draw a red line to visualize the trend and distribution
z = np.polyfit(y_test, y_pred, 1)
p = np.poly1d(z)
plt.plot(y_test, p(y_test), "r--")

plt.show()

# ----------------------Forecast manually 

# Define the forecasted x input
fc_data = pd.DataFrame([[2.000, 24500, 1.5, 50000]], columns=['bond_gap', 'VNDUSD', 'VN_INT_3M', 'OMO_cum'])

# Predict the value
y_pred1 = model.predict(fc_data)

print(f"Predicted value1: {y_pred1[0]}")


# ------------------------Retrain
# Create a linear regression model
model2 = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)

# Train the model
model2.fit(np.array(y_pred).reshape(-1, 1), y_test)

# Make predictions
y_pred2 = model2.predict(np.array(y_pred1).reshape(-1, 1))

print(f"Predicted value2: {y_pred2[0]}")




# --------------------------------------------------------Linear regression to predict




import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import numpy as np


data_file = r'C:\Users\417776\Downloads\Python\fin\macro data\sample1.csv'

df = pd.read_csv(data_file)

# Define features (X) and target (y)
X = df[['bond_gap', 'VNDUSD', 'VN_INT_3M' , 'OMO_cum']]
y = df['index']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')

# Print model coefficients
print('Coefficients:', model.coef_)
print('Intercept:', model.intercept_)

# Visual Scatter plot of y_test vs y_pred
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel('Actual values (y_test)')
plt.ylabel('Predicted values (y_pred)')
plt.title('Actual vs Predicted values')

# Draw a red line to visualize the trend and distribution
z = np.polyfit(y_test, y_pred, 1)
p = np.poly1d(z)
plt.plot(y_test, p(y_test), "r--")

plt.show()

# ----------------------Forecast manually
# Define the forecasted x input
fc_data = pd.DataFrame([[2.500, 24500, 1.0, 150000],
                        [2.000, 24500, 2.0, 0],
                        [1.500, 24500, 3.0, -150000]], columns=['bond_gap', 'VNDUSD', 'VN_INT_3M', 'OMO_cum'])

# Predict the value
y_pred1 = model.predict(fc_data)

print(f"Predicted value: {y_pred[0]}")

# ------------------------Retrain
# Create a linear regression model
model2 = LinearRegression()

# Train the model
model2.fit(np.array(y_pred).reshape(-1, 1), y_test)

# Make predictions
y_pred2 = model2.predict(np.array(y_pred1).reshape(-1, 1))

print(f"Predicted value: {y_pred2[0]}")




-------------------------------------------------------------return json file





from flask import Flask, jsonify
import pandas as pd
import numpy as np

app = Flask(__name__)

@app.route('/get-data', methods=['GET'])
def get_data():
    # Load CSV file using pandas
    csv_file = 'sample1.csv'  # Specify your CSV file path
    df = pd.read_csv(csv_file)

    # Replace NaN values with None (so they become null in JSON)
    df = df.replace({np.nan: None})

    # Convert the DataFrame to a dictionary (list of records)
    data = df.to_dict(orient='records')  # Returns a list of dictionaries

    # Wrap the data under the key "macrodata"
    response = {
        "macrodata": data
    }

    # Return the wrapped data as JSON
    return jsonify(response)

if __name__ == '__main__':
    app.run(debug=True)


