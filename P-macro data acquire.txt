import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import pandas as pd
import json
import numpy as np


# ----------------------GET DATA HISTORICAL 10VNBY

url_10 = "https://www.worldgovernmentbonds.com/wp-admin/admin-ajax.php?action=jsonStoricoBond&area=58&dateRif=2099-12-31&durata=120&key=d1RHRzY4aE16dU9TaXRvSndPWVBEdz09"

response_10 = requests.get(url_10).text
data_10 = json.loads(response_10)  # Parse the JSON data directly
# print(data_dict)

# Convert to DataFrame
df_10 = pd.DataFrame.from_dict(data_10['quote'], orient='index', columns=['date', 'bond_10y'])

# Convert timestamp to readable date format
df_10['date'] = pd.to_datetime(df_10['date'], unit='ms')


df_combined_10f = pd.concat([df_10], ignore_index=True).dropna(subset=['bond_10y'])

# REMOVE DUPLICATE LIST
df_combined_10f = df_combined_10f.drop_duplicates(subset=['date'], keep='last')

# ----------------------GET DATA FORECAST 10VNBY


url_10f = ('http://www.worldgovernmentbonds.com/bond-forecast/vietnam/10-years/')
r_10f = requests.get(url_10f)
# print(r.text)

# Get time forecast
web_content_10f = BeautifulSoup(r_10f.text, 'lxml')
web_content_10f = web_content_10f.find('div', {"class" : 'w3-responsive'})
web_content_10f = web_content_10f.find('thead').text
# print(web_content)

# Use regular expression to find lines with dates in MMM YYYY format
pattern = r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) \d{4}'
dates_10f = re.findall(pattern, web_content_10f)

# Convert month-year strings to datetime objects with day set to 1
date_objects_10f = [datetime.strptime(date, '%b %Y') for date in dates_10f]

date_10f = []
# Print the converted date objects
for date_obj_10f in date_objects_10f:
    date_10f.append(date_obj_10f)

date_10f = pd.DataFrame({'date': date_10f})
# print(date)

# Get number forecast
web_number_10f = BeautifulSoup(r_10f.text, 'lxml')
web_number_10f = web_number_10f.find('div', {"class" : 'w3-responsive'})
web_number_10f = web_number_10f.find('tbody')
web_number_10f = web_number_10f.find('tr').text

# # Use regular expression to find lines with '%'
pattern = r'^\d+\.\d+%$'
lines_with_percent_10f = re.findall(pattern, web_number_10f, re.MULTILINE)

# Print the matching lines
bond_y_10f = []
for line in lines_with_percent_10f:
    bond_y_10f.append(line)

bond_y_10f = pd.DataFrame({'bond_10y': bond_y_10f})
# print(bond_y)

# Consolidate the two DataFrames into one
consolidated_df_10f = pd.concat([date_10f, bond_y_10f], axis=1)

# Remove the '%' symbol
consolidated_df_10f['bond_10y'] = consolidated_df_10f['bond_10y'].str.replace('%', '')

# Convert the 'bond_y' column to numeric
consolidated_df_10f['bond_10y'] = pd.to_numeric(consolidated_df_10f['bond_10y'])

# COMBINE DATAFRAME

df_combined_10f = pd.concat([df_combined_10f, consolidated_df_10f], ignore_index=True).dropna(subset=['bond_10y'])

# REMOVE DUPLICATE LIST
df_combined_10f = df_combined_10f.drop_duplicates(subset=['date'], keep='last')

# ----------------------GET DATA HISTORICAL 02VNBY

url_2 = "https://www.worldgovernmentbonds.com/wp-admin/admin-ajax.php?action=jsonStoricoBond&area=58&dateRif=2099-12-31&durata=24&key=dENuNGVDdERQdUh6ZEhqZTBOWnN5Zz09"

response_2 = requests.get(url_2).text
data_2 = json.loads(response_2)  # Parse the JSON data directly
# print(data_dict)

# Convert to DataFrame
df_2 = pd.DataFrame.from_dict(data_2['quote'], orient='index', columns=['date', 'bond_2y'])

# Convert timestamp to readable date format
df_2['date'] = pd.to_datetime(df_2['date'], unit='ms')

df_combined_2f = pd.concat([df_2], ignore_index=True).dropna(subset=['bond_2y'])

# REMOVE DUPLICATE LIST
df_combined_2f = df_combined_2f.drop_duplicates(subset=['date'], keep='last')

# ----------------------GET DATA FORECAST 2VNBY


url_2f = ('http://www.worldgovernmentbonds.com/bond-forecast/vietnam/2-years/')
r_2f = requests.get(url_2f)
# print(r.text)

# Get time forecast
web_content_2f = BeautifulSoup(r_2f.text, 'lxml')
web_content_2f = web_content_2f.find('div', {"class" : 'w3-responsive'})
web_content_2f = web_content_2f.find('thead').text
# print(web_content)

# Use regular expression to find lines with dates in MMM YYYY format
pattern = r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) \d{4}'
dates_2f = re.findall(pattern, web_content_2f)

# Convert month-year strings to datetime objects with day set to 1
date_objects_2f = [datetime.strptime(date, '%b %Y') for date in dates_2f]

date_2f = []
# Print the converted date objects
for date_obj_2f in date_objects_2f:
    date_2f.append(date_obj_2f)

date_2f = pd.DataFrame({'date': date_2f})
# print(date)

# Get number forecast
web_number_2f = BeautifulSoup(r_2f.text, 'lxml')
web_number_2f = web_number_2f.find('div', {"class" : 'w3-responsive'})
web_number_2f = web_number_2f.find('tbody')
web_number_2f = web_number_2f.find('tr').text

# # Use regular expression to find lines with '%'
pattern = r'^\d+\.\d+%$'
lines_with_percent_2f = re.findall(pattern, web_number_2f, re.MULTILINE)

# Print the matching lines
bond_y_2f = []
for line in lines_with_percent_2f:
    bond_y_2f.append(line)

bond_y_2f = pd.DataFrame({'bond_2y': bond_y_2f})
# print(bond_y)

# Consolidate the two DataFrames into one
consolidated_df_2f = pd.concat([date_2f, bond_y_2f], axis=1)

# Remove the '%' symbol
consolidated_df_2f['bond_2y'] = consolidated_df_2f['bond_2y'].str.replace('%', '')

# Convert the 'bond_y' column to numeric
consolidated_df_2f['bond_2y'] = pd.to_numeric(consolidated_df_2f['bond_2y'])

# COMBINE DATAFRAME

df_combined_2f = pd.concat([df_combined_2f, consolidated_df_2f], ignore_index=True).dropna(subset=['bond_2y'])

# REMOVE DUPLICATE LIST
df_combined_2f = df_combined_2f.drop_duplicates(subset=['date'], keep='last')

# --------------------COMBINE DATAFRAME

# JOIN 2 DATAFRAME
df_combined = pd.merge(df_combined_10f, df_combined_2f, how="left"\
              , left_on=['date']\
              , right_on=['date']\
              , validate="1:1")

# -----------------------GET DATA OMO

# Load your data
omo_df = pd.read_csv('C:\\Users\\417776\\Downloads\\Python\\fin\\macro data\\OMO.csv')
omo_df['date'] = pd.to_datetime(omo_df['Date'])
# omo_df = omo_df[['date', 'OMO', 'OMO_balance']]
omo_df = omo_df[['date', 'OMO']]

# Load your data update
omo_df2 = pd.read_csv('C:\\Users\\417776\\Downloads\\Python\\fin\\macro data\\OMO_update.csv')    
omo_df2['date'] = pd.to_datetime(omo_df2['Date'])
omo_df2[['Volume_buy', 'Volume_sell']] = omo_df2[['Volume_buy', 'Volume_sell']].replace(',', '', regex=True).apply(pd.to_numeric)
omo_df2['OMO'] = omo_df2['Volume_buy']-omo_df2['Volume_sell']

# omo_df2['OMO_balance'] = omo_df2['Balance']

# omo_df2 = omo_df2[['date', 'OMO', 'OMO_balance']]
omo_df2 = omo_df2[['date', 'OMO']]


# COMBINE DATAFRAME
combined_omo_df = pd.concat([omo_df, omo_df2], ignore_index=True)

# REMOVE DUPLICATE LIST
combined_omo_df = combined_omo_df.drop_duplicates(subset=['date'], keep='last')

# Calculate cumulative sum
combined_omo_df['OMO_cum'] = np.cumsum(combined_omo_df['OMO'])

# JOIN 2 DATAFRAME
df_combined3 = pd.merge(df_combined, combined_omo_df, how="left"\
              , left_on=['date']\
              , right_on=['date']\
              , validate="1:1")

# df_combined3['OMO_balance'] = df_combined3['OMO_balance'].fillna(method='ffill').fillna(method='bfill')
df_combined3['OMO_cum'] = df_combined3['OMO_cum'].fillna(method='ffill').fillna(method='bfill')
    

# -----------------------GET DATA VNDUSD

from pycoingecko import CoinGeckoAPI

cg = CoinGeckoAPI()

# Get a list of all supported coins
coins_list = cg.get_coins_list()

# Filter coins based on symbol
coins_list = filter(lambda coin: coin['id'] in ["usd"], coins_list)    

# Initialize an empty list to store OHLC data
all_coin_data = []

# Loop through each coin
for coin in coins_list:
    coin_id = coin['id']
    coin_symbol = coin['symbol']

    # Get OHLC data for the past 14 days (adjust as needed)
    ohlc = cg.get_coin_ohlc_by_id(id=coin_id, vs_currency='vnd', days='365')

    # Create a dataframe for the coin's OHLC data
    ohlc_df = pd.DataFrame(ohlc, columns=["date", "open", "high", "low", "close"])
    ohlc_df["date"] = pd.to_datetime(ohlc_df["date"], unit="ms")
    ohlc_df["symbol"] = coin_symbol
    # print(ohlc_df)

    # Append to the list of all coin data
    all_coin_data.append(ohlc_df)

# Concatenate dataframes for all coins
forex_pd = pd.concat(all_coin_data, ignore_index=True)
forex_pd['VNDUSD'] = forex_pd['close']
forex_pd = forex_pd[['date', 'VNDUSD']]


# # JOIN 2 DATAFRAME
# df_combined2 = pd.merge(df_combined, forex_pd, how="left"\
#               , left_on=['date']\
#               , right_on=['date']\
#               , validate="1:1")

# GET DATA VNDUSD_excel
# Load your data
vndusd_df = pd.read_csv('C:\\Users\\417776\\Downloads\\Python\\fin\\macro data\\VNDUSD_consol.csv')
vndusd_df['date'] = pd.to_datetime(vndusd_df['date'])
vndusd_df = vndusd_df[['date', 'VNDUSD']]

# COMBINE DATAFRAME
combined_vndusd_df = pd.concat([forex_pd, vndusd_df], ignore_index=True).dropna(subset=['VNDUSD'])

# REMOVE DUPLICATE LIST
combined_vndusd_df = combined_vndusd_df.drop_duplicates(subset=['date'], keep='last')

# JOIN 2 DATAFRAME
df_combined3 = pd.merge(df_combined3, combined_vndusd_df, how="left"\
              , left_on=['date']\
              , right_on=['date']\
              , validate="1:1")

df_combined3['VNDUSD'] = df_combined3['VNDUSD'].fillna(method='ffill').fillna(method='bfill')
    
# -----------------------GET DATA VNINT_consol

# Load old data
vnint_df = pd.read_csv('C:\\Users\\417776\\Downloads\\Python\\fin\\macro data\\VNInterest_consol.csv')
vnint_df['date'] = pd.to_datetime(vnint_df['date'])
vnint_df = vnint_df[['date', 'VN_INT_3M']]

# GET current data
# Load your data
vnint_df2 = pd.read_csv('C:\\Users\\417776\\Downloads\\Python\\fin\\macro data\\OMO_update.csv') 
vnint_df2['date'] = pd.to_datetime(vnint_df2['Date'])
vnint_df2['VN_INT_3M'] = vnint_df2['Int3M']
vnint_df2 = vnint_df2[['date', 'VN_INT_3M']]

# COMBINE DATAFRAME
combined_vnint_df = pd.concat([vnint_df, vnint_df2], ignore_index=True).dropna(subset=['VN_INT_3M'])

# REMOVE DUPLICATE LIST
combined_vnint_df = combined_vnint_df.drop_duplicates(subset=['date'], keep='last')


# JOIN 2 DATAFRAME
df_combined3 = pd.merge(df_combined3, combined_vnint_df, how="left"\
              , left_on=['date']\
              , right_on=['date']\
              , validate="1:1")

df_combined3['VN_INT_3M'] = df_combined3['VN_INT_3M'].fillna(method='ffill').fillna(method='bfill')


# -----------------------GET DATA VNINDEX

# Load your data update
index_df = pd.read_excel('C:\\Users\\417776\\Downloads\\Python\\fin\\macro data\\vnin.xlsx', sheet_name = 'Sheet1')    
index_df['date'] = pd.to_datetime(index_df['Date/Time'])
index_df = index_df[['date', 'index']]

# JOIN 2 DATAFRAME
df_combined3 = pd.merge(df_combined3, index_df, how="left"\
              , left_on=['date']\
              , right_on=['date']\
              , validate="1:1")

df_combined3['index'] = df_combined3['index'].fillna(method='ffill').fillna(method='bfill')

# Filter
# df_combined3 = df_combined3[df_combined3.date >= "16-09-2019"]
df_combined3 = df_combined3[df_combined3.date >= "22-09-2022"]
df_combined3['bond_gap'] = df_combined3['bond_10y']-df_combined3['bond_2y']

# Print the consolidated DataFrame
print(df_combined3)
# df_combined3.to_csv("sample1.csv")

correlation_OMOvsVNDUSD = df_combined3[['OMO_cum', 'VNDUSD']].corr().iloc[0, 1]
correlation_IntvsBondgap = df_combined3[['VN_INT_3M', 'bond_gap']].corr().iloc[0, 1]
correlation_BondgapvsIndex = df_combined3[['bond_gap', 'index']].corr().iloc[0, 1]
OMO_1MChange = df_combined3['OMO_cum'] - df_combined3['OMO_cum'].shift(30)
VNDUSD_1MChangeAmt = df_combined3['VNDUSD'] - df_combined3['VNDUSD'].shift(30)
VNDUSD_1MChange = df_combined3['VNDUSD']/df_combined3['VNDUSD'].shift(30)-1
VN_INT_1MChange = df_combined3['VN_INT_3M']-df_combined3['VN_INT_3M'].shift(30)
Bondgap_1MChange = df_combined3['bond_gap']-df_combined3['bond_gap'].shift(30)


# --------------------- Measurement
# OMO Measurement
if OMO_1MChange.iloc[-1] > 0:
    OMO_Measure = f"Tích cực"
elif OMO_1MChange.iloc[-1] < 0:
    OMO_Measure = f"Tiêu cực"
else:
    OMO_Measure = "Trung lập"
    
# OMO comment
if OMO_1MChange.iloc[-1] > 0:
    OMO_comment = f"NHNN bơm ròng 1 tháng gần nhất {OMO_1MChange.iloc[-1]/1000} ngàn tỷ đồng qua nghiệp vụ thị trường mở"
elif OMO_1MChange.iloc[-1] < 0:
    OMO_comment = f"NHNN hút ròng 1 tháng gần nhất {OMO_1MChange.iloc[-1]/1000} ngàn tỷ đồng qua nghiệp vụ thị trường mở"
else:
    OMO_comment = "Không ghi nhận hoạt động của NHNN trên nghiệp vụ thị trường mở trong 1 tháng gần nhất"

# VNDUSD comment
if VNDUSD_1MChange.iloc[-1] >= 0:
    VNDUSD_comment = f"Tỷ giá USDVND tăng {VNDUSD_1MChangeAmt.iloc[-1]/1000} ngàn đồng tương ứng tăng {round(VNDUSD_1MChange.iloc[-1]*100,2)}% trong 1 tháng gần nhất"
else: 
    VNDUSD_comment = f"Tỷ giá USDVND giảm {VNDUSD_1MChangeAmt.iloc[-1]/1000} ngàn đồng tương ứng giảm {round(VNDUSD_1MChange.iloc[-1]*100,2)}% trong 1 tháng gần nhất"


# VN_INT comment
if VN_INT_1MChange.iloc[-1] >= 0:
    VN_INT_comment = f"Lãi suất liên ngân hàng tăng {round(VN_INT_1MChange.iloc[-1],2)}% trong 1 tháng gần nhất"
else: 
    VN_INT_comment = f"Lãi suất liên ngân hàng giảm {round(VN_INT_1MChange.iloc[-1],2)}% trong 1 tháng gần nhất"

# Bond gap Measurement
if Bondgap_1MChange.iloc[-1] >= 0:
    Bondgap_measure = f"Tích cực"
else: 
    Bondgap_measure = f"Tiêu cực"

# Bond gap comment
if Bondgap_1MChange.iloc[-1] >= 0:
    Bondgap_comment = f"Chênh lệch Lợi suất trái phiếu 10 năm so với 2 năm tăng {round(Bondgap_1MChange.iloc[-1],2)}% trong 1 tháng gần nhất"
else: 
    Bondgap_comment = f"Chênh lệch Lợi suất trái phiếu 10 năm so với 2 năm giảm {round(Bondgap_1MChange.iloc[-1],2)}% trong 1 tháng gần nhất"

# correlation_OMOvsVNDUSD comment
if correlation_OMOvsVNDUSD < -0.5 and VNDUSD_1MChange.iloc[-1] >= 0:
    correlation_OMOvsVNDUSD_comment = f"Hoạt động bơm hút ròng qua nghiệp vụ thị trường mở của NHNN có tương quan nghịch với biến động của tỷ giá USDVND. Khi tỷ giá USDVND tăng, NHNN có xu hướng hút ròng tiền qua nghiệp vụ thị trường mở, nhằm giảm cung tiền VND từ đó giúp tăng giá trị VND để cân đối lại tỷ giá. Cụ thể mức độ tương quan giữa Tỷ giá USDVND với Giá trị bơm hút ròng qua nghiệp vụ thị trường mở là {round(correlation_OMOvsVNDUSD,2)}"
elif correlation_OMOvsVNDUSD < -0.5 and VNDUSD_1MChange.iloc[-1] < 0:
    correlation_OMOvsVNDUSD_comment = f"Hoạt động bơm hút ròng qua nghiệp vụ thị trường mở của NHNN có tương quan nghịch với biến động của tỷ giá USDVND. Khi tỷ giá USDVND giảm, NHNN có xu hướng bơm ròng tiền qua nghiệp vụ thị trường mở, nhằm tăng cung tiền VND từ đó giúp giảm giá trị VND để cân đối lại tỷ giá. Cụ thể mức độ tương quan giữa Tỷ giá USDVND với Giá trị bơm hút ròng qua nghiệp vụ thị trường mở là {round(correlation_OMOvsVNDUSD,2)}"
else:
    correlation_OMOvsVNDUSD_comment = ""

# correlation_IntvsBondgap measure
if correlation_IntvsBondgap < -0.5 and correlation_BondgapvsIndex > 0.5 and Bondgap_1MChange.iloc[-1] > 0 and VN_INT_1MChange.iloc[-1] < 0:
    correlation_IntvsBondgap_measure = "Tích cực"
elif correlation_IntvsBondgap < -0.5 and correlation_BondgapvsIndex > 0.5 and Bondgap_1MChange.iloc[-1] < 0 and VN_INT_1MChange.iloc[-1] > 0:
    correlation_IntvsBondgap_measure = "Tiêu cực"
elif correlation_IntvsBondgap < -0.5 and correlation_BondgapvsIndex > 0.5 and Bondgap_1MChange.iloc[-1] > 0:
    correlation_IntvsBondgap_measure = "Tích cực"
elif correlation_IntvsBondgap < -0.5 and correlation_BondgapvsIndex > 0.5 and Bondgap_1MChange.iloc[-1] < 0:
    correlation_IntvsBondgap_measure = "Tiêu cực"
else:
    correlation_IntvsBondgap_measure = ""

# correlation_IntvsBondgap comment
if correlation_IntvsBondgap < -0.5 and correlation_BondgapvsIndex > 0.5 and Bondgap_1MChange.iloc[-1] > 0 and VN_INT_1MChange.iloc[-1] < 0:
    correlation_IntvsBondgap_comment = f"Lãi suất liên ngân hàng quan hệ tương quan nghịch với Chênh lệch lợi suất trái phiếu 10 năm và 2 năm (Hệ số tương quan là {round(correlation_IntvsBondgap,2)}). Khi lãi suất liên ngân hàng giảm, Chênh lệch lợi suất trái phiếu 10 năm và 2 năm có xu hướng tăng. Theo mô hình phân tích, Vnindex có tương quan thuận với Chênh lệch lợi suất trái phiếu 10 năm và 2 năm với mức tương quan {round(correlation_BondgapvsIndex,2)}. Từ đó cho thấy, Lãi suât liên ngân hàng giảm là tín hiệu tích cực với Vnindex"
elif correlation_IntvsBondgap < -0.5 and correlation_BondgapvsIndex > 0.5 and Bondgap_1MChange.iloc[-1] < 0 and VN_INT_1MChange.iloc[-1] > 0:
    correlation_IntvsBondgap_comment = f"Lãi suất liên ngân hàng quan hệ tương quan nghịch với Chênh lệch lợi suất trái phiếu 10 năm và 2 năm (Hệ số tương quan là {round(correlation_IntvsBondgap,2)}). Khi lãi suất liên ngân hàng tăng, Chênh lệch lợi suất trái phiếu 10 năm và 2 năm có xu hướng giảm. Theo mô hình phân tích, Vnindex có tương quan thuận với Chênh lệch lợi suất trái phiếu 10 năm và 2 năm với mức tương quan {round(correlation_BondgapvsIndex,2)}. Từ đó cho thấy, Lãi suât liên ngân hàng tăng là tín hiệu tiêu cực với Vnindex"
elif correlation_IntvsBondgap < -0.5 and correlation_BondgapvsIndex > 0.5 and Bondgap_1MChange.iloc[-1] > 0:
    correlation_IntvsBondgap_comment = f"Theo mô hình phân tích, Vnindex có tương quan thuận với Chênh lệch lợi suất trái phiếu 10 năm và 2 năm với mức tương quan {round(correlation_BondgapvsIndex,2)}. Từ đó cho thấy, Chênh lệch lợi suất trái phiếu 10 năm và 2 năm tăng là tín hiệu tích cực với Vnindex"
elif correlation_IntvsBondgap < -0.5 and correlation_BondgapvsIndex > 0.5 and Bondgap_1MChange.iloc[-1] < 0:
    correlation_IntvsBondgap_comment = f"Theo mô hình phân tích, Vnindex có tương quan thuận với Chênh lệch lợi suất trái phiếu 10 năm và 2 năm với mức tương quan {round(correlation_BondgapvsIndex,2)}. Từ đó cho thấy, Chênh lệch lợi suất trái phiếu 10 năm và 2 năm giảm là tín hiệu tiêu cực với Vnindex"
else:
    correlation_IntvsBondgap_comment = ""

# # Main
# print(OMO_Measure) #---Tích cực -> positive_color , Tiêu cực -> negative
# print(OMO_1MChange.iloc[-1]/1000) #---Giảm -> negative_color , tăng -> positive, DVT: ngàn tỷ đồng, Diễn giải: Bơm hút ròng tín phiếu
# print(round(VNDUSD_1MChange.iloc[-1]*100,2)) #---Tăng -> negative_color , giảm -> positive, DVT: %, Diễn giải: Thay đổi tỷ giá USDVND

# print(correlation_IntvsBondgap_measure) #---Tích cực -> positive_color , Tiêu cực -> negative
# print(round(VN_INT_1MChange.iloc[-1],2)) #---Giảm -> positive_color , tăng -> negative, DVT: %, Diễn giải: Thay đổi lãi suất liên ngân hàng
# print(round(Bondgap_1MChange.iloc[-1],2)) #---Giảm -> negative_color , tăng -> positive, DVT: %, Diễn giải: Thay đổi Chênh lệch LSTP \n 10 năm và 2 năm
# # Note: Thay đổi 1 tháng gần nhất

# # Explanation
# print(OMO_Measure)
# print(OMO_comment)
# print(VNDUSD_comment)
# print(correlation_OMOvsVNDUSD_comment)
# # Note: Mô hình phân tích dựa trên dữ liệu quan sát 2 năm gần nhất

# print(Bondgap_measure)
# print(Bondgap_comment)
# # print(VN_INT_comment)
# print(correlation_IntvsBondgap_measure)
# print(correlation_IntvsBondgap_comment)
# # Note: Mô hình phân tích dựa trên dữ liệu quan sát 2 năm gần nhất

data_measure = {
    'Report_date': [datetime.now().strftime('%Y%m%d')],
    
    'OMO_Measure_Main': [OMO_Measure], #---notification when changing this field
    'OMO_1MChange_Main': [OMO_1MChange.iloc[-1]/1000],
    'VNDUSD_1MChange_Main': [round(VNDUSD_1MChange.iloc[-1]*100,2)],
    
    'correlation_IntvsBondgap_measure_Main': [correlation_IntvsBondgap_measure], #---notification when changing this field
    'VN_INT_1MChange_Main': [round(VN_INT_1MChange.iloc[-1],2)],
    'Bondgap_1MChange_Main': [round(Bondgap_1MChange.iloc[-1],2)],
    
    'OMO_Measure': [OMO_Measure], 
    'OMO_comment': [OMO_comment],
    'VNDUSD_comment': [VNDUSD_comment],
    'correlation_OMOvsVNDUSD_comment': [correlation_OMOvsVNDUSD_comment],
    
    'Bondgap_measure': [Bondgap_measure],
    'Bondgap_comment': [Bondgap_comment],
    'correlation_IntvsBondgap_measure': [correlation_IntvsBondgap_measure],
    'correlation_IntvsBondgap_comment': [correlation_IntvsBondgap_comment]
}

# Create a DataFrame from the dictionary
data_summary = pd.DataFrame(data_measure)

print(data_summary)
data_summary.to_csv("sample3.csv", encoding='utf-8-sig', index=False)




# --------------------------------------------------------Non-Linear regression to predict





import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from sklearn.inspection import partial_dependence
import numpy as np

data_file = r'C:\Users\417776\Downloads\Python\fin\macro data\sample1.csv'


# Load your DataFrame (replace 'your_data.csv' with your actual data file)
df = pd.read_csv(data_file)
# df = df.head(2000)
print(df)

# Separate features (X) and target (y)
X = df[['bond_gap', 'VNDUSD', 'VN_INT_3M' , 'OMO_cum' ]]
y = df['index']
# print(X)

# Initialize Random Forest model
model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)

# Walk-forward validation
tscv = TimeSeriesSplit(n_splits=5)
for train_index, test_index in tscv.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    print(f"Fold MSE: {mse:.4f}")

# Train the final model on the entire dataset
model.fit(X, y)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate model performance (optional)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.2f}")

# Print the most important features
important_features_dict = {}
for idx, val in enumerate(model.feature_importances_):
    important_features_dict[idx] = val

important_features_list = sorted(important_features_dict, key=important_features_dict.get, reverse=True)
print(f'The most important features are: {important_features_list}')

# Print all the attributes
print(f'The coefficient of determination R^2 of the prediction is: {model.score(X_test, y_test)}')
# print(f'The number of trees in the forest is: {model.n_estimators}')
# print(f'The function to measure the quality of a split is: {model.criterion}')
# print(f'The maximum depth of the tree is: {model.max_depth}')
# print(f'The minimum number of samples required to split an internal node is: {model.min_samples_split}')
# print(f'The minimum number of samples required to be at a leaf node is: {model.min_samples_leaf}')
# print(f'The output for a given input is: {model.predict(X_test)}')
# print(f'The list of decision trees in the forest is: {model.estimators_}')

# ------------Sensitivity analysis
# Drop duplicate rows
X_copy = X.copy()
X_unique = X_copy.drop_duplicates()

# Predict RenewalRate for each row in X_unique
predictions_index = model.predict(X_unique)

# # Add the predicted RenewalRate values to X_unique
X_unique['predicted_index'] = predictions_index

print(X_unique)
X_unique.to_csv("predicted_index.csv")

# # Visualize relationships (optional)
# plt.scatter(X_test['VNDUSD'], y_test, label='index')
# plt.scatter(X_test['VNDUSD'], y_pred, label='predicted_index', color='red', marker='x')
# plt.xlabel('VNDUSD')
# plt.ylabel('index')
# plt.title('VNDUSD vs. index')
# plt.legend()
# plt.show()


# Visual Scatter plot of y_test vs y_pred
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel('Actual values (y_test)')
plt.ylabel('Predicted values (y_pred)')
plt.title('Actual vs Predicted values')

# Draw a red line to visualize the trend and distribution
z = np.polyfit(y_test, y_pred, 1)
p = np.poly1d(z)
plt.plot(y_test, p(y_test), "r--")

plt.show()

# ----------------------Forecast manually 

# Define the forecasted x input
fc_data = pd.DataFrame([[2.000, 24500, 1.5, 50000]], columns=['bond_gap', 'VNDUSD', 'VN_INT_3M', 'OMO_cum'])

# Predict the value
y_pred1 = model.predict(fc_data)

print(f"Predicted value1: {y_pred1[0]}")


# ------------------------Retrain
# Create a linear regression model
model2 = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)

# Train the model
model2.fit(np.array(y_pred).reshape(-1, 1), y_test)

# Make predictions
y_pred2 = model2.predict(np.array(y_pred1).reshape(-1, 1))

print(f"Predicted value2: {y_pred2[0]}")




# --------------------------------------------------------Linear regression to predict




import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import numpy as np


data_file = r'C:\Users\417776\Downloads\Python\fin\macro data\sample1.csv'

df = pd.read_csv(data_file)

# Define features (X) and target (y)
X = df[['bond_gap', 'VNDUSD', 'VN_INT_3M' , 'OMO_cum']]
y = df['index']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')

# Print model coefficients
print('Coefficients:', model.coef_)
print('Intercept:', model.intercept_)

# Visual Scatter plot of y_test vs y_pred
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel('Actual values (y_test)')
plt.ylabel('Predicted values (y_pred)')
plt.title('Actual vs Predicted values')

# Draw a red line to visualize the trend and distribution
z = np.polyfit(y_test, y_pred, 1)
p = np.poly1d(z)
plt.plot(y_test, p(y_test), "r--")

plt.show()

# ----------------------Forecast manually
# Define the forecasted x input
fc_data = pd.DataFrame([[2.500, 24500, 1.0, 150000],
                        [2.000, 24500, 2.0, 0],
                        [1.500, 24500, 3.0, -150000]], columns=['bond_gap', 'VNDUSD', 'VN_INT_3M', 'OMO_cum'])

# Predict the value
y_pred1 = model.predict(fc_data)

print(f"Predicted value: {y_pred[0]}")

# ------------------------Retrain
# Create a linear regression model
model2 = LinearRegression()

# Train the model
model2.fit(np.array(y_pred).reshape(-1, 1), y_test)

# Make predictions
y_pred2 = model2.predict(np.array(y_pred1).reshape(-1, 1))

print(f"Predicted value: {y_pred2[0]}")




-------------------------------------------------------------return json file





from flask import Flask, jsonify
import pandas as pd
import numpy as np

app = Flask(__name__)

@app.route('/get-data', methods=['GET'])
def get_data():
    # Load CSV file using pandas
    csv_file = 'sample1.csv'  # Specify your CSV file path
    df = pd.read_csv(csv_file)

    # Replace NaN values with None (so they become null in JSON)
    df = df.replace({np.nan: None})

    # Convert the DataFrame to a dictionary (list of records)
    data = df.to_dict(orient='records')  # Returns a list of dictionaries

    # Wrap the data under the key "macrodata"
    response = {
        "macrodata": data
    }

    # Return the wrapped data as JSON
    return jsonify(response)

if __name__ == '__main__':
    app.run(debug=True)


